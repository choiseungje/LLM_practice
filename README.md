# Llama Fast API Server

<img src="./res/image/screenshot.png" style="border: 0.1px solid grey; border-radius: 10px"/>

## Installation

### FastAPI

#### Windows
```bash
pip install fastapi uvicorn websockets huggingface_hub
```

#### macOS
```bash
pip3 install fastapi uvicorn websockets huggingface_hub
```


### Llama-cpp-python

#### Windows + CUDA
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

#### Windows + CPU
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
```

#### macOS + Metal
```bash
pip3 install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal
```
